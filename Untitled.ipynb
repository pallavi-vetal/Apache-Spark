{"metadata":{"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <h3>Spark Dataframe, Datasets and RDDs</h3>","metadata":{},"id":"5897de03-d9d6-4e0c-bb15-65c53e4779d3"},{"cell_type":"markdown","source":"<table>\n    <th>\n        RDD\n    </th>\n    <th>\n        Dataframe\n    </th>\n    <th>\n        Dataset\n    </th>\n    <tr>\n        <td>\n            RDD is a collection of immutable, fault-tolerant, distributed collection of data\n            which can be processed in parallel.\n        </td>\n        <td>\n            Distributed collection of data with named columns\n        </td>\n        <td>\n            Dataset is distributed collection of data. Provides benefits of dataset + rdd\n        </td>\n     </tr> \n    <tr>\n        <td>\n            Schema needs to be defined manually\n        </td>\n        <td>\n            Auto infers schema\n        </td>\n        <td>\n            Auto infers schema\n        </td>\n    </tr>\n    <tr>\n        <td>\n            No optimization provided. Programmer needs to write optimized code.\n        </td>\n        <td>\n            Uses catalyst optimizer for query optimization.\n        </td>\n        <td>\n            Uses catalyst optimizer for query optimization.\n        </td>\n    </tr>\n        <tr>\n        <td>\n           RDD uses java/kyro serializer to encode data which is expensive operation \n            as it needs to shuffle both data and structure between nodes in a cluster.\n        </td>\n        <td>\n            There is no java serializer and enocoderl. \n            Serialization happens in memory in binary format which results in faster processing.\n        </td>\n        <td>\n            Dataset uses a specialized Encoder to serialize the objects for processing or transmitting over the network. While both encoders and standard serialization are responsible for turning an object into bytes, encoders are code generated dynamically and use a format that allows Spark to perform many operations like filtering, sorting and hashing without deserializing the bytes back into an object.\n        </td>\n    </tr>\n        \n</table>","metadata":{},"id":"855268d8-8fb0-466e-9b97-eda9e2833a15"},{"cell_type":"markdown","source":"<h4>Global Temporary Views</h4>\n<p>\n    <small>\n        When you create temp views, they will be available till your spark-session is alive. \n        If you want to have a temporary view that is shared among all sessions and \n        keep alive until the Spark application terminates, \n you can create a global temporary view. \n        Global temporary view is tied to a system preserved database global_temp\n        <code>df.createOrReplaceGlobalTempView(\"Global Temp View Name\")</code>\n    </small>\n</p>","metadata":{},"id":"2ac5d408-27f3-4922-9f85-331f9b3563cc"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"5e1a4fcb-b7fd-40e7-b1fc-b41131b7973f"}]}